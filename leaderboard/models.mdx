---
title: 'Leaderboard'
description: 'Complete MCPBench model performance rankings with detailed metrics and analysis'
---

# MCPBench Model Leaderboard

Average task resolution success rate for top and select models on MCPBench's dataset of 127 tasks across multiple domains.

<CardGroup cols={4}>
  <Card title="Total Models" icon="cpu">
    <Stat>15</Stat>
  </Card>
  <Card title="Total Tasks" icon="list">
    <Stat>127</Stat>
  </Card>
  <Card title="Top Success Rate" icon="trophy">
    <Stat>95.2%</Stat>
  </Card>
  <Card title="Avg Success Rate" icon="trending-up">
    <Stat>87.3%</Stat>
  </Card>
</CardGroup>

## Overall Rankings

| Rank | Model | Organization | Release Date | Success Rate | System Admin | Security | Data Science | Research |
|------|-------|--------------|--------------|--------------|--------------|----------|--------------|----------|
| ðŸ¥‡ 1 | **Claude 4 Sonnet** | Anthropic | May 2024 | <Badge variant="success">95.2%</Badge> | <Badge variant="success">96.1%</Badge> | <Badge variant="success">94.8%</Badge> | <Badge variant="success">94.5%</Badge> | <Badge variant="success">95.7%</Badge> |
| ðŸ¥ˆ 2 | **GPT-4o** | OpenAI | May 2024 | <Badge variant="success">92.8%</Badge> | <Badge variant="success">93.2%</Badge> | <Badge variant="success">92.1%</Badge> | <Badge variant="success">93.4%</Badge> | <Badge variant="success">92.6%</Badge> |
| ðŸ¥‰ 3 | **Gemini Pro** | Google | Dec 2023 | <Badge variant="success">89.4%</Badge> | <Badge variant="success">90.1%</Badge> | <Badge variant="success">88.7%</Badge> | <Badge variant="success">89.8%</Badge> | <Badge variant="success">89.1%</Badge> |
| 4 | **Claude 3.5 Sonnet** | Anthropic | Jun 2024 | <Badge variant="primary">88.7%</Badge> | <Badge variant="primary">89.2%</Badge> | <Badge variant="primary">88.1%</Badge> | <Badge variant="primary">88.9%</Badge> | <Badge variant="primary">88.5%</Badge> |
| 5 | **GPT-4 Turbo** | OpenAI | Nov 2023 | <Badge variant="primary">87.9%</Badge> | <Badge variant="primary">88.4%</Badge> | <Badge variant="primary">87.2%</Badge> | <Badge variant="primary">88.1%</Badge> | <Badge variant="primary">87.8%</Badge> |
| 6 | **Gemini Ultra** | Google | Dec 2023 | <Badge variant="primary">86.2%</Badge> | <Badge variant="primary">86.8%</Badge> | <Badge variant="primary">85.7%</Badge> | <Badge variant="primary">86.4%</Badge> | <Badge variant="primary">86.1%</Badge> |
| 7 | **Claude 3 Opus** | Anthropic | Mar 2024 | <Badge variant="primary">84.8%</Badge> | <Badge variant="primary">85.3%</Badge> | <Badge variant="primary">84.2%</Badge> | <Badge variant="primary">84.9%</Badge> | <Badge variant="primary">84.7%</Badge> |
| 8 | **GPT-4** | OpenAI | Mar 2023 | <Badge variant="primary">83.1%</Badge> | <Badge variant="primary">83.6%</Badge> | <Badge variant="primary">82.4%</Badge> | <Badge variant="primary">83.3%</Badge> | <Badge variant="primary">83.0%</Badge> |
| 9 | **Claude 3 Sonnet** | Anthropic | Mar 2024 | <Badge variant="primary">81.7%</Badge> | <Badge variant="primary">82.2%</Badge> | <Badge variant="primary">81.1%</Badge> | <Badge variant="primary">81.9%</Badge> | <Badge variant="primary">81.6%</Badge> |
| 10 | **Gemini Pro Vision** | Google | Dec 2023 | <Badge variant="primary">79.4%</Badge> | <Badge variant="primary">79.9%</Badge> | <Badge variant="primary">78.7%</Badge> | <Badge variant="primary">79.6%</Badge> | <Badge variant="primary">79.2%</Badge> |

## Performance by Category

<CardGroup cols={2}>
  <Card title="System Administration" icon="server">
    <Stat>45 tasks</Stat>
    Leading model: Claude 4 Sonnet (96.1%)
  </Card>
  <Card title="Security" icon="shield">
    <Stat>28 tasks</Stat>
    Leading model: Claude 4 Sonnet (94.8%)
  </Card>
  <Card title="Data Science" icon="brain">
    <Stat>32 tasks</Stat>
    Leading model: Claude 4 Sonnet (94.5%)
  </Card>
  <Card title="Research" icon="search">
    <Stat>7 tasks</Stat>
    Leading model: Claude 4 Sonnet (95.7%)
  </Card>
</CardGroup>

## Recent Submissions

<CardGroup cols={2}>
  <Card title="Claude 4 Sonnet" icon="star">
    **Latest Update**: December 15, 2024  
    **Improvement**: +2.1% overall performance  
    **New Features**: Enhanced reasoning capabilities
  </Card>
  <Card title="GPT-4o" icon="zap">
    **Latest Update**: December 10, 2024  
    **Improvement**: +1.8% in security tasks  
    **New Features**: Better code analysis
  </Card>
</CardGroup>

## Methodology

Our evaluation methodology ensures fair and comprehensive assessment:

<Steps>
  <Step title="Task Selection">
    Tasks are selected to represent real-world terminal operations across multiple domains.
  </Step>
  <Step title="Environment Setup">
    Each model is evaluated in a standardized Linux environment with consistent tooling.
  </Step>
  <Step title="Automated Evaluation">
    Results are automatically verified using predefined success criteria and test cases.
  </Step>
  <Step title="Human Review">
    Complex tasks undergo additional human review to ensure accurate assessment.
  </Step>
  <Step title="Statistical Analysis">
    Performance metrics are calculated using robust statistical methods.
  </Step>
</Steps>

## Submit Your Results

<CardGroup cols={2}>
  <Card title="Submit New Model" icon="plus" href="/leaderboard/submit-results">
    Submit evaluation results for a new model or updated version.
  </Card>
  <Card title="View Submission Guidelines" icon="book" href="/leaderboard/submit-results">
    Learn about our evaluation process and submission requirements.
  </Card>
</CardGroup>

<Info>
  Results are updated regularly as new submissions are received and verified. Check back frequently for the latest rankings.
</Info>

<Warning>
  **Benchmark Integrity**: All tasks include canary tokens to prevent contamination of training data. The benchmark should never appear in AI model training corpora.
</Warning>